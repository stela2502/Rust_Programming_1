# Use the Data class to implement a kmeans algorithm

Now you know how to create a package and even know how to load numeric tables into Rust.
But how do we interact with this table and how does a kmeans algorithm work?

I could of cause explain that in detail here, but that would not help you a lot - or?
Instead I show you how I would find this info nowadays: using ChatGPT.
So why not try it here, too.

This was my conversation with ChatGPT:

```
Hi Chatgpt. Can you tell me how to code kmeans in Rust if the data is stored in 2d ndarrays  and the rows should be clustered.

Sure, I can help you with that! Here's an implementation of the KMeans algorithm in Rust using the ndarray crate to handle the 2D data:

rust

use ndarray::{Array2, Axis};

fn kmeans(data: &Array2<f64>, k: usize) -> Array2<f64> {
    let n = data.nrows();
    let d = data.ncols();
    let mut centroids = data.slice(s![..k, ..]).to_owned();
    let mut labels = Array2::zeros((n, 1));

    loop {
        let mut new_labels = Array2::zeros((n, 1));
         for i in 0..n {
            let mut min_distance = f64::INFINITY;
            for j in 0..k {
                let distance = (data.slice(s![i, ..]) - centroids.slice(s![j, ..])).norm();
                if distance < min_distance {
                    min_distance = distance;
                    new_labels[[i, 0]] = j as f64;
                }
            }
        }
        if new_labels == labels {
            break;
        }
        labels.assign(&new_labels);
        for j in 0..k {
            let mask = labels.iter().enumerate().filter(|(_, &l)| l == j as f64)
                .map(|(i, _)| i).collect::<Vec<_>>();
            let cluster_data = data.select(Axis(0), &mask);
            if !cluster_data.is_empty() {
                centroids.slice_mut(s![j, ..]).assign(&cluster_data.mean_axis(Axis(0)).unwrap());
            }
        }
    }
    labels
}

This implementation takes as input an Array2<f64> representing the data, and an integer k representing the number of clusters to find. It returns an Array2<f64> of the same shape as the input data, containing the cluster labels for each row.

The algorithm works as follows:

    Initialize centroids to k randomly chosen rows from the input data.
    Loop until convergence:
    a. Assign each row to the cluster with the closest centroid.
    b. Update the centroid of each cluster to be the mean of its assigned rows.
    c. If the cluster assignments haven't changed, exit the loop.
    Return the cluster labels for each row.

The implementation should be fairly self-explanatory, but I'll provide some additional details:

    We use the norm() method to compute the Euclidean distance between a row of data and a centroid.
    The slice() method is used to extract subsets of the data and centroids arrays.
    The select() method is used to extract the rows of data belonging to a given cluster.
    The mean_axis() method is used to compute the mean of the rows belonging to a given cluster.

Hope that helps! Let me know if you have any questions.
````

## Implement the kmeans algorithm using the AI output?

Sure - why not. Let's recapitulate what the kmers algorithm should do and check if we find the respective lines in the code.

The data should already be in memory and it should be a ndarray. Here it is obviously called "data". But I would not like to have an Array2\<f64\> back.
This function should rather return the cluster identities for each row as a Vec\<usize\>. Keep that in mind for later.

The next step in the kmeans algorithm is to select a random set of initial 'centromers'. This step is totally missing in the AI function.
To select a random set of starting rows we need a random number generator. In Rust this works like that:

```{rust}
use rand::Rng;
let mut rng = rand::thread_rng();
rng.gen_range(0..100); // gives a random number in this range.
```

Next we loop until convergence is reached. That is fine. 
BUT we do not want the program to end in an endless loop. 
Add another variable called max_it:usize and use this in a 'for' loop.


# Let's walk through:

Change the data type to the type we have in our read_data, the return value to a Vec\<usize\> and add a way to **randomly** select the initial set of centroids.


```{rust}
use ndarray::ArrayBase;
use rand::Rng;


fn kmeans(data: &ArrayBase<ndarray::OwnedRepr<f64>, Dim<[usize; 2]>>, k: usize) -> Vec<usize> {
    let n = data.nrows();
    let d = data.ncols();
    let mut rng = rand::thread_rng();

    let mut centroids = Array::zeros((k, data.len_of(Axis(1))));
    for i in  0..k{
        let sample_idx = rng.gen_range(0..n);
        centroids.row_mut(i).assign(&self.data.row(sample_idx));
    }
    let mut labels = Vec<usize>::with_capacity(n);

    .
    .
    .
}
```

You see the creation of the initial labels changed quite a bit from the AI generated ``let mut centroids = data.slice(s![..k, ..]).to_owned();``. 
We instead use a for loop that creates k random row ids and asignes the data from the respective row to the i'th centrioid.

Hence we now get exactly k **random** rows of the data and not just the first k rows as in the AI code.
At least the part of how to get data from the data object into the centroids object is more or less a copy from ChatGPT.


## Should we have more functions?

The next step is to calculate the closest centroid for each row of the data.

We will need this functionality also in the loop. As we use the same functionality more than once I would implement a function to do that.
This function should compare the centroids to every row of the data and I would call it assign_labels:


```{rust}
fn assign_labels(data: &ArrayBase<ndarray::OwnedRepr<f64>, Dim<[usize; 2]>>, centroids: &ArrayBase<ndarray::OwnedRepr<f64>, Dim<[usize; 2]>> ) -> Vec<usize> {
    let n = data.nrows();
    let mut labels = Vec<usize>::with_capacity( n );
    
    for id in 0..n{
        let mut min_distance = f64::INFINITY;
        for j in 0..k {
            let distance = (data.slice(s![i, ..]) - centroids.slice(s![j, ..])).norm();
            if distance < min_distance {
                min_distance = distance;
                labels[i] = j;
            }
        }
    }
    labels
}
```

The assign_labels function is mainly a straight copy from the initial AI result.
In short we use each data row once and calculate the distance to all centroids.
If the distance we get is smaller than all others before we assign the centroid id to the lable for this data row.

Let's add this logic to the main function:

```{rust}

fn kmeans(data: &ArrayBase<ndarray::OwnedRepr<f64>, Dim<[usize; 2]>>, k: usize) -> Vec<usize> {
    let n = data.nrows();
    let d = data.ncols();
    let mut rng = rand::thread_rng();

    let mut centroids = Array::zeros((k, data.len_of(Axis(1))));
    for i in  0..k{
        let sample_idx = rng.gen_range(0..n);
        centroids.row_mut(i).assign(&self.data.row(sample_idx));
    }
    let mut labels = assign_labels( data, centroids );

    .
    .
    .
}

```

## Add a function to calculate the new centroids

The next time we get the centroids will be more complicated.
I like functions as one can also create specific tests for each function.

So lets add a "calculate_centroids" function:

```{rust}
fn calculate_centroids( data: &ArrayBase<ndarray::OwnedRepr<f64>, Dim<[usize; 2]>>, labels:Vec<usize>, 
            k:usize, &mut centroids: &ArrayBase<ndarray::OwnedRepr<f64>, Dim<[usize; 2]>> ) {

    for j in 0..k {
        let mask = labels.iter().enumerate()
            .filter(|(_, &l)| l == j )
            .map(|(i, _)| i)
            .collect::<Vec<_>>();
        let cluster_data = data.select(Axis(0), &mask);
        if !cluster_data.is_empty() {
            centroids.slice_mut(s![j, ..]).assign(&cluster_data.mean_axis(Axis(0)).unwrap());
        }
    } 
}
```

Oops - that one is complicated!

```{rust}
let mask = labels.iter().enumerate()
  .filter(|(_, &l)| l == j )
  .map(|(i, _)| i)
  .collect::<Vec<_>>(); 
```


Let's break it down:

* `` labels.iter().enumerate() `` 
This creates the data we work on: The ids and variables in the label vector (``.iter()``: values and ``.enumerate()``: with the ids ).

* ``filter(|(_, &l)| l == j )`` 
This filter only uses the second variable, the label id and checks it against the cluster 'j'. Only labels that have this id reach the next step.

* ``map(|(i, _)| i) `` 
Here we only get the id and value of rows that are in cluster 'j'. So we simply need to 'cut out' the row id here and ignore the value.


So the mask variable in the end contains the row_ids that are in cluster j.

We collect this data into a new let cluster_data and if that is not empty we calculate the new centroid as the mean of this cluster_data. Simple?

In the calculate centroids I now also give the centroids object to the function. This way we do not need to create a temporary variable holding the centroids but instead change the values in the variable owned by the kmeans function. That is quite slick and we should probably also have done that in the assign_labels function. Would it even make sense there?


Let's add this logic to the main function:

```{rust}
fn kmeans(data: &ArrayBase<ndarray::OwnedRepr<f64>, Dim<[usize; 2]>>, k: usize, max_it:usize) -> Vec<usize> {
    let n = data.nrows();
    let d = data.ncols();
    let mut rng = rand::thread_rng();

    let mut centroids = Array::zeros((k, data.len_of(Axis(1))));
    for i in  0..k{
        let sample_idx = rng.gen_range(0..n);
        centroids.row_mut(i).assign(&self.data.row(sample_idx));
    }
    let mut labels = assign_labels( data, centroids );
    let mut new_labels:Vec<usize>;

    for it in 0..max_it{
        calculate_centroids( data, labels, k, centroids );
        new_labels = assign_labels( data, centroids );
        if new_labels == labels{
            print("converged after {it} itherations");
            break;
        }
        labels = new_labels;
    }
    labels
}

```

# And the TESTING!


As we now have so many nice functions that could be tested - lets add some tests:

To be able to test this we now neeed some data in this. :-(
Let's combine the kmeans functionality with the Data object we had as an example before.

Copy the read_data folder and name it kmeans.
Now you need to change the Cargo.toml: read_data to kmeans.

You could compile it and check if it is still working.

Add the kmeans functionality to the Data class or to the main.rs script.
This will take some time - I will help you, but there will be no help in this document.

We go on with the testing here:


```{rust}

#[cfg(test)]
mod tests {

    use crate::data::Data;
    use ndarray::Axis;
    use ndarray::s;

    #[test]
    fn check_assign_labels() {
        let data = Data::read_file( &"testData/CellexalVR_TestData_tsne.csv".to_string(), ',' );
        let centroids = data.data.slice(s![..10, ..]).to_owned();
        let kmeans = data.assign_labels( &centroids );
        let mut min = usize::MAX;
        let mut max = usize::MIN;
        for k in &kmeans {
            if min > *k{
                min = *k;
            }
            if max < *k{
                max = *k;
            }
        }
        assert_eq!( [min, max], [0,9] );
    }

     #[test]
    fn check_kmeans() {

        let mut data = Data::read_file( &"testData/CellexalVR_TestData_tsne.csv".to_string(), ',' );
        data.scale();
        let kmeans = data.kmeans( 15, 15000 );
        assert_eq!( kmeans.len(), data.data.len_of(Axis(0)) );

        //let exp:Vec<usize> = vec![0; data.data.len_of(Axis(0)) ];

        let mut min = usize::MAX;
        let mut max = usize::MIN;
        for k in kmeans {
            if min > k{
                min = k;
            }
            if max < k{
                max = k;
            }
        }
        assert_eq!( [min , max ], [0, 14] );
    }

}

```

Add these tests BEFORE you implement the functions. This way you have a simple test to check if it does work ;-)

Please try to integrate the kmeans function into the read_data crate.

You can either add the kmeans logics to the main.rs script or you augment the read_table Data class.

This will be a hands on training session. And not a too simple one. Please do not look into the kmeans_example folder.
This would ruin the exercise for you.

Instead copy the read_data folder from the github repo and rename the project to kmeans_<your_name> in the Cargo.toml.
Think about where you want to add the functions and do it. Compile and dig through the compiler errors.
Or you try to clean out the problems first.

### Compile and test the kmeans program

```{bash,  error=TRUE}
cd kmeans_example
cargo build -r 2>&1 | sed -r "s/\x1B\[([0-9]{1,2}(;[0-9]{1,2})?)?[m|K]//g"
target/release/kmeans -h
```
```{bash,  error=TRUE}
cd kmeans_example
target/release/kmeans -d testData/CellexalVR_TestData_tsne.csv -c 15 -s ','
```

Ok that did not take long. But how fast would R do that?

Did this make sense? Let's plot this data using R:

```{r}
clusters = read.delim('kmeans_example/testData/Clustering.txt', sep=",", row.names=1, header=F)
head(clusters)
data = read.delim( 'kmeans_example/testData/CellexalVR_TestData_tsne.csv', sep=",", row.names=1)
head(data)

plot( data[,1:2], col=rainbow(15)[clusters[,1]+1], pch=16)
```

But what about speed? Can we compare that to R? First check R like we did it in the R programming 1:

```{r}

# from ChatGPT:

e.dist <- function(x, y) {
  sqrt(sum((x - y)^2))
}


kmeans_r = function( yyc, k=8 ){
    n = nrow(yyc)
    centroids <- yyc[sample(1:n, k, replace=F),] # make a vector of random clusters.

    dists <- matrix(0, nrow = n, ncol = k) # make an empty matrix to fill with distances.

    clusters <- NULL # make an empty variable to catch the clusters in the loop below
    old_clusters <- rep(0, n)
    for(iteration in 1:100){ # 100 iterations

      for(gene in 1:n){
        for(cl in 1:k){
          dists[gene, cl] <- e.dist(yyc[gene,], centroids[cl,]) # for each gene calculate the distance to each centroid (cl).
        }
      }

      clusters <- apply(dists, 1, which.min) # assign a  cluster according to which centroid is nearest
      if ( all.equal(old_clusters, clusters) ==TRUE ){
        return (clusters);
      }
      old_clusters =clusters

      for(cl in 1:k){
        centroids[cl,] <- apply(yyc[which(clusters == cl),], 2, mean) # define new centroids.
      }
    }
    clusters
}
```

Let's time that call:


```{r}
start_time <- Sys.time()
#data = read.delim( 'kmeans_example/testData/CellexalVR_TestData_tsne.csv', sep=",", row.names=1)
#data = read.delim( './Spellman_Yeast_Cell_Cycle.tsv', sep=",", row.names=1)
df <- data.frame(age = c(18, 21, 22, 24, 26, 26, 27, 30, 31, 35, 39, 40, 41, 42, 44, 46, 47, 48, 49, 54),
    spend = c(10, 11, 22, 15, 12, 13, 14, 33, 39, 37, 44, 27, 29, 20, 28, 21, 30, 31, 23, 24)
)
cl = kmeans_r( df, 3)
end_time <- Sys.time()
print(end_time - start_time)
plot( df[,1:2], col=rainbow(15)[cl], pch=16)
```


OK even this tiny example (20 rows) took more time than what the Rust program needed for a much bigger matrix (1654 rows).
So this simple R code is far slower than the Rust code. But what about the kmeans implementation in R? This should be highly optimized.

Does the R internal kmeans (likely programmed in C) outperform Rust?


```{r}
start_time <- Sys.time()
data = read.delim( 'kmeans_example/testData/CellexalVR_TestData_tsne.csv', sep=",", row.names=1)
cl = kmeans( data, 15)
end_time <- Sys.time()
print(end_time - start_time)
plot( data[,1:2], col=rainbow(15)[cl$cluster], pch=16)
```



=======
# Take home message:

Rust totally outclasses simple R code and ends up in the same speed range if optimized (c or c++) code is used in R.

But trust me - you do not want to write optimized c or c++ code!


